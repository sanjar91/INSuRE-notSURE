# Progress Report (insert date here)
## Overview
(insert brief overview of efforts made)

## Outcomes
(brief overview of outcomes - what did you achieve?)

also list them out like this:
* outcome 1
* outcome 2

We further refined our methodology during this milestone.  We learned that the previous calculation we used for inter-rater reliability (IRR) did not effectively accomplish the goal we were trying to meet.  We have revised our methodology in the following ways, explained in more detail below:
 * Recalculated inter-rater reliability over the entire control set for each technology domain
 * Based on the level of agreeability, calculated the percent agreement for each individual control
 * Used a combmination of percent agreement and a consensus-based approach to determine final set of controls for each domain.

First, we recalculated the inter-rater reliability.  Following additional research, we learned that the IRR calculation is significantly more reliable when there are more items being measured with it.  As a result, we altered our methodology to calculate IRR over the entire set of controls instead of individual controls to get a better idea of the degree to which we agreed overall.

With our new IRR calculation, we used the Fleiss' Kappa calculation.  We had five raters (*n* = 5) rate 98 controls (N = 98) as either *Yes*, the control is measurable in the given technology domain, or *No*, the control is not measurable in the given technology domain (*k* = 2).

## Hinderances
(insert brief discussion of challenges encountered)
